# NeuroNaut - Autonomous LLM Agent with MPC

We propose an **autonomous lifelong-learning agent** that uses a private local LLM to explore environments, build skills, and accomplish tasks via secure tool use. Inspired by MineDojo's *Voyager* agent and Anthropic’s Twitch/Mario demo, NeuroNaut continuously generates or receives goals (e.g. compile an app, defeat a game boss, build a Swift package from a README), reasons about them, retrieves past experiences from memory, and calls tools (e.g. game actions, CLI commands, code execution) to iteratively solve them.

Over time, the agent builds a **Skill Library**—a set of reusable code routines or tool sequences—indexed by embeddings and re-applied in similar situations. The system includes **self-verification** and optional dual-agent setup (planner + critic), and runs **entirely locally** on Raspberry Pi or Apple Silicon machines using open models like **Qwen**, **Mistral**, and **Mixtral**, powered by `llama.cpp`.

---

## Core Design Principles

* **Fully self-hosted** on Raspberry Pi / Apple Silicon (M1/M2) / Intel Mac
* **Modular** agent architecture with MCP (Model Context Protocol) tool interface
* **LLM-Orchestrated Planning** using local open-source models
* **Voyager-style skill acquisition**, indexing and reuse
* **Self-improving, persistent memory** with vector search and RAG
* **Secure execution** with sandboxed CLI and file access

---

## System Components

### 1. LLM Core

* Local model inference via `llama.cpp` (Metal/CPU)
* Default: **Mistral Small 3** (128K context, native function-calling)
* Optional: **Qwen 3-30B A3B**, **Mixtral 8x22B** for deeper reasoning
* Distributed via **Petals** or **Paddler** load balancer (multi-device)

### 2. Agent Loop (Controller)

* Runs as a Swift or Python asyncio process
* Loops: `goal` → `LLM prompt` → `tool call (JSON)` → `result` → `verify / retry`
* Tracks task progress, handles retries, timeouts, and tool safety quotas
* Optional **dual-agent loop**: planner + critic (e.g. CRITIC-style)

### 3. MCP Tool Servers (Isolated Environment Interfaces)

* **Filesystem/Terminal**: read/write/edit files, run CLI commands (safe subset only)
* **Code Executor**: sandboxed compile/run of Swift or Python (via Docker or subprocess)
* **Telegram-MCP**: interact with game chatbots via inline buttons
* **Web Search / Fetch**: perform safe network queries (MCP-fetch)
* **Memory Tool**: key-value memory and embedding store (optional)

### 4. Memory / RAG Store

* **Chroma** or **Qdrant** for persistent memory
* Stores: skill descriptions, logs, embeddings of goals/plans/code
* Retrieved via embedding-based search per task step
* Skills and logs persist across sessions

### 5. Skill Library (Inspired by Voyager)

* Stores each learned solution: { name, description, code, toolchain, success metadata, embedding }
* Queried via embedding similarity + keywords
* Includes self-rating and success frequency for prioritization

---

## Loop Overview

1. **Receive goal**: e.g. "Build a Swift package from README"
2. **Recall** similar skills and logs from vector memory
3. **Plan** multi-step actions with LLM (e.g. list steps)
4. **Act** by emitting tool calls (`tool_call` via MCP)
5. **Observe** results (stdout, logs, chat reply, etc.)
6. **Critique & retry** if needed
7. **Learn**: if successful, generate reusable skill
8. **Store** code in Skill Library, log in memory store
9. **Repeat** for next goal

---

## Swift Integration

* Swift is used where performance, safety, or static typing is critical:

  * MCP server for filesystem access
  * MCP bash command wrapper
  * Swift code compilation and log reading
* Swift package generated by the agent can itself be managed, compiled, debugged
* Swift-based CLI agent is a possible future upgrade, replacing the Python loop

---

## Safety & Autonomy

* Timeouts and retry counters per tool
* CLI command allowlist (no `rm -rf` or destructive ops)
* Output validator (LLM critic or regex check)
* Persistent session log: recovery after restart

---

## Optional Enhancements

* **Auto-curriculum scheduler**: feed harder tasks after success
* **Progress tracker**: like a README checklist or .json file
* **GUI dashboard or log visualizer**
* **Fine-tuning successful behaviors as LoRAs (optional)**

---

## Comparison: Voyager, Claude-Mario, AutoGPT, Qwen-Agent

| Feature                  | Voyager          | Claude-Mario     | AutoGPT       | Qwen-Agent   | **NeuroNaut**       |
| ------------------------ | ---------------- | ---------------- | ------------- | ------------ | ------------------- |
| Local execution          | ❌ Cloud-based    | ✅ Local (Twitch) | ✅ w/ setup    | ✅ (optional) | ✅ Fully local       |
| MCP support              | ❌ No             | ✅ Yes (Claude)   | ❌ No          | ✅ Native     | ✅ Yes               |
| Critic/self-verification | ✅ Yes            | ✅ Yes            | ✅ Yes         | ✅ Optional   | ✅ Optional (dual)   |
| Skill Library            | ✅ Auto-learned   | ❌                | ✅ With memory | ✅ In-code    | ✅ Embedding-indexed |
| Custom tool integration  | ❌ Minecraft-only | ✅ Partial        | ✅ CLI, web    | ✅ via MCP    | ✅ CLI, Files, Web   |
| Swift compatibility      | ❌                | ❌                | ❌             | ❌            | ✅ Yes               |

---

## Summary

**NeuroNaut** is a modular, self-contained agent that runs entirely on your local hardware. It connects open-source LLMs (Qwen, Mistral) to MCP tools (CLI, filesystem, Telegram), learns skills over time, and reuses them to complete new goals faster. It’s inspired by Voyager and Anthropic’s MCP vision, but implemented with your preferred tools—Swift, Apple Silicon, Docker, and vector memory. It can be extended with more tools, dual-agent critique, and a curriculum engine for long-term growth.

> This document forms the vision. See `README.md` for implementation checklist.
