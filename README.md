# ğŸ§  **NeuroNaut** â€“ Autonomous Localâ€‘First LLM Agent Platform

**Selfâ€‘improving agents Â· Onâ€‘device & edge AI Â· Secure tool integration**

---

![Swift](https://img.shields.io/badge/Swift-f05138?logo=swift&logoColor=white)  ![Python](https://img.shields.io/badge/Python-3776ab?logo=python&logoColor=white)  ![llama.cpp](https://img.shields.io/badge/llama.cpp-00bfa6?logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTIiIGhlaWdodD0iMTIiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+PHBhdGggZD0iTTAgMGgxMnYxMkgweiIgZmlsbD0iI2ZmZiIgLz48L3N2Zz4=)  ![Docker](https://img.shields.io/badge/Docker-0db7ed?logo=docker&logoColor=white)  ![AppleÂ Silicon](https://img.shields.io/badge/AppleÂ Silicon-b2b2b2?logo=apple&logoColor=white)  ![MCP](https://img.shields.io/badge/ModelÂ ContextÂ Protocol-7348ff)  ![Telethon](https://img.shields.io/badge/Telethon-2895d3)  ![ChromaDB](https://img.shields.io/badge/Chroma-ff6e6e)

---

**NeuroNaut** is a fully selfâ€‘hosted, lifelongâ€‘learning agent inspired by *Voyager*.  It runs entirely on your own Appleâ€‘Silicon or Intel Macs, explores new environments, discovers reusable **skills**, and executes real tools through the **Model Context Protocol (MCP)**.

*No cloud APIs.Â  No data leaves your LAN.Â  Just pure, private AI power.*

---

## ğŸš€ QuickÂ Pitch

NeuroNaut loops forever:

1. **Perceive** â€“Â Read structured state from MCP tools (e.g. Telegram game chat, filesystem, compiler logs).
2. **Plan &Â Think** â€“Â Use a local LLM (default **MistralÂ SmallÂ 3**, fallback **MixtralÂ 8Ã—22Â B** / **QwenÂ 3â€‘A3B**) with RAGÂ memory.
3. **Act** â€“Â Emit JSON function calls.  The controller invokes Telegram buttons, compiles Swift, runs shell commandsâ€¦
4. **Learn** â€“Â If the action succeeds, summarise it into a new **skill**.  Store code + docstring + embedding â†’ SkillÂ Library.
5. **Repeat** â€“Â Automatic curriculum keeps pushing to harder goals (beat RPG boss, ship Swift package, etc.).

Over time NeuroNaut builds a personal toolbox of code routines and action macros â€“ ready to solve ever larger tasks.

---

## ğŸ—ï¸ SystemÂ Architecture

* **LLMÂ Core** â€“ llama.cppâ€‘backed local model pool (MistralÂ SmallÂ 3 default). Loadâ€‘balanced via **Paddler** or **Petals**.
* **MemoryÂ /RAG** â€“Â Vector DB (Chroma/Qdrant) + optional Knowledgeâ€‘Graph MCP.
* **SkillÂ Library** â€“Â Proven code snippets/functions indexed by embeddings.
* **MCPÂ Tools** â€“Â Isolated services (Telegram, codeâ€‘sandbox, filesystem, etc.).
* **AgentÂ Loop** â€“Â Python `asyncio` controller orchestratingÂ LLM â‡„ Tools â‡„ Memory.

```mermaid
flowchart LR
  %% === Agent core ===
  subgraph Agent["Agent (local)"]
    A[Agent Loop] -->|prompt| L(LLM Core)
    A --> M(Memory / RAG)
    A --> S(Skill Library)
    L -->|tool_call JSON| C(MCP Controller)
    C -->|result + logs| A
  end

  %% === Tool side ===
  subgraph "MCP Tools"
    TG[Telegramâ€‘MCP]
    CE[Code Executor]
    FS[Filesystem / Terminal]
    UT[Utility / Fetch ...]
  end

  %% === Connections ===
  C -->|invoke| TG
  C -->|invoke| CE
  C -->|invoke| FS
  C -->|invoke| UT

  TG -->|result| C
  CE -->|result| C
  FS -->|result| C
  UT -->|result| C
```

---

## ğŸ› ï¸ TechÂ Stack

| Layer | Component | Notes |
|-------|-----------|-------|
| **LLMÂ Inference** | **llama.cpp** (AppleÂ Silicon) | 4â€‘bit GGUF models; GPU/Metal acceleration |
|Â  | **Paddler / Petals** | Distribute queries across multiple Macs |
| **DefaultÂ Model** | **MistralÂ SmallÂ 3Â (24â€¯B)** | 150â€¯tok/s, native functionâ€‘calling, 128â€¯K ctx, vision variant |
| **Fallbacks** | MixtralÂ 8Ã—22â€¯B Â· QwenÂ 3â€‘30Bâ€‘A3B | Higher reasoning / longer context |
| **Orchestration** | LangChainÂ / LlamaIndex | Pureâ€‘local pipelines |
| **VectorÂ DB** | ChromaÂ /Â Qdrant | Stores memories & skill metadata |
| **MCPÂ Servers** | Telegramâ€‘MCP Â· Codeâ€‘Executor Â· Filesystem Â· iTerm | Secure, sandboxed tool wrappers |
| **Languages** | Swift Â· Python Â· Bash | Swift for exec tools, Python for orchestration |
| **Ops** | Docker Â· Git Â· systemd | Containerised services, autoâ€‘restart |

---

### ğŸ” Languageâ€‘Model Selection Matrix

| # | Requirement | Best Fit | Why |
|---|-------------|---------|-----|
| 1 | Tool/function JSON | **MistralÂ SmallÂ 3** | OpenAIâ€‘style schema, zero adapter code |
| 2 | Fast inference (<Â 32â€¯GB) | **MistralÂ SmallÂ 3** | 11â€¯GB Q4\_K\_M, 150â€¯tok/s |
| 3 | Deep reasoning | MixtralÂ 8Ã—22â€¯B | +10â€¯% higher coding benchmarks |
| 4 | >Â 65â€¯K context | QwenÂ 3â€‘30Bâ€‘A3B | 131â€¯K tokens |
| 5 | Native vision | MistralÂ SmallÂ 3Â Vision | Builtâ€‘in SigLIP encoder |

Switch engines via `LLM_BACKEND` env var.  GGUF models live in `./models`.

---

## ğŸ“š Openâ€‘Source BuildingÂ Blocks

* **llama.cpp** â€“ local LLM runtime (Apple Metal / CPU)
* **Paddler** â€“ Rust balancer for multiple llama.cpp servers
* **Telethonâ€‘MCP** â€“ Telegram API wrapper as MCP tool
* **modelcontextprotocol/servers** â€“ reference *codeâ€‘executor*, *filesystem*, *memory* tools
* **Chroma / Qdrant** â€“ vector stores for RAG & skills
* **LangChain / LlamaIndex** â€“ orchestration layer
* **Docker** â€“ containerise MCP servers, DB, dashboards

All under permissive licences (ApacheÂ 2.0, MIT).  100â€¯% selfâ€‘hosted.

---

## ğŸ—ºï¸ DevelopmentÂ Roadmap &Â Checklist

- [ ] **LLMÂ InferenceÂ Setup** â€“ build & test llama.cpp (LLaMAâ€‘2Â 7â€¯B)
- [ ] **ClusterÂ Balancing** â€“ deploy Paddler / Petals pool
- [ ] **Telegramâ€‘MCP** â€“ connect bot, test `press_button`
- [ ] **Codeâ€‘ExecutorÂ MCP** â€“ sandboxed Swift/Python compileâ€‘run
- [ ] **FilesystemÂ MCP** â€“ read/write code, logs
- [ ] **VectorÂ DB** â€“ install Chroma, integrate embeddings
- [ ] **SkillÂ LibraryÂ v0** â€“ JSON/SQLite schema, embedding index
- [ ] **AgentÂ LoopÂ MVP** â€“ prompt â†’ JSONÂ tool â†’ result loop
- [ ] **RPGÂ AutoplayÂ PoC** â€“ reach levelÂ 2 automatically
- [ ] **Selfâ€‘Verification & Rewards** â€“ goal checker helper
- [ ] **Swiftâ€‘PackageÂ Builder** â€“ generate, compile, iterate
- [ ] **MemoryÂ Replay & Longâ€‘term RAG**
- [ ] **SkillÂ Consolidation** â€“ summarise & store reusable skills
- [ ] **SafetyÂ Rails** â€“ timeouts, retry, sandbox quotas
- [ ] **ProgressÂ File** â€“ agent marks tasks complete for next session

> **Progress is persistent** â€“ the agent resumes where the checklist stops.

---

## ğŸ“ˆÂ HowÂ ItÂ LearnsÂ (QuickÂ Example)

1. **Goal**Â â†’ â€œDefeat Forest GoblinÂ Boss.â€  
2. LLM plans â†’ calls `press_button{"label":"Hunt"}`  
3. Telegramâ€‘MCP presses *Hunt*; returns fight log.  
4. LLM interprets HP & damage, decides to heal â†’ `press_button{"label":"Use Potion"}`.  
5. After victory, controller prompts: *"Write a reusable skill."*  
6. LLM emits Python function `def fight_forest_boss(): ...` + docstring.  
7. Skill stored in library with embedding.  Next time, similar goal triggers retrieval & reuse.
